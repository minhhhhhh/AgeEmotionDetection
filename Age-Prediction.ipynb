{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2519a08-8e18-4a18-93a5-5c76e664240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd57189a-403b-476e-8cbc-bc8a79caa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load APPA-REAL dataset\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\", force_remount=True)\n",
    "data_dir = \"/Users/home/Downloads/appa-real-release/\"\n",
    "\n",
    "labels_path = os.path.join(data_dir, \"gt_avg_train.csv\")  # Adjust based on dataset\n",
    "\n",
    "# Read CSV containing file names and age labels\n",
    "df = pd.read_csv(labels_path)\n",
    "\n",
    "# Load and preprocess images\n",
    "images = []\n",
    "ages = []\n",
    "for index, row in df.iterrows():\n",
    "    image_path = os.path.join(data_dir, \"train\", row['file_name'])  # Adjust folder structure\n",
    "    if os.path.exists(image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (224, 224))  # Resize for CNN\n",
    "        images.append(image)\n",
    "        ages.append(row['apparent_age_avg'])  # Use the average apparent age\n",
    "\n",
    "images = np.array(images) / 255.0  # Normalize pixel values\n",
    "ages = np.array(ages)\n",
    "\n",
    "# Split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, ages, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0418c2b0-4106-4f0a-815d-de3ccbeb9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 308ms/step - loss: 452.9085 - mae: 16.4513 - val_loss: 165.0536 - val_mae: 10.1100\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 336ms/step - loss: 183.9905 - mae: 10.3663 - val_loss: 146.6058 - val_mae: 9.4922\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 288ms/step - loss: 156.9636 - mae: 9.7970 - val_loss: 139.5662 - val_mae: 9.0879\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 319ms/step - loss: 156.2671 - mae: 9.5437 - val_loss: 142.8239 - val_mae: 9.1061\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 303ms/step - loss: 143.2968 - mae: 9.2012 - val_loss: 130.5045 - val_mae: 8.7766\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 331ms/step - loss: 134.2624 - mae: 8.9745 - val_loss: 129.6397 - val_mae: 8.7018\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 311ms/step - loss: 131.4221 - mae: 8.9241 - val_loss: 128.8203 - val_mae: 8.7142\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 315ms/step - loss: 135.9735 - mae: 8.9957 - val_loss: 125.9212 - val_mae: 8.5515\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 292ms/step - loss: 129.2290 - mae: 8.8849 - val_loss: 133.2575 - val_mae: 8.9228\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 290ms/step - loss: 126.0646 - mae: 8.7669 - val_loss: 124.8344 - val_mae: 8.5139\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNetV2\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the full model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)  # Regression output for predicting age\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/Users/home/Downloads/AgeModels/age_detected_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8a092-4fb2-45c4-a224-b6e2ffda661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"/Users/home/Downloads/AgeModels/age_detected_model.keras\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def predict_age(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face = image[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (224, 224)) / 255.0\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "\n",
    "        # Predict age\n",
    "        age = model.predict(face)[0][0]\n",
    "        age_text = f\"Age: {int(age)}\"\n",
    "\n",
    "        # Draw a rectangle and label on the image\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(image, age_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Age Detection\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test the function\n",
    "predict_age(\"/Users/home/Downloads/appa-real-release/test/005613.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812705b-22c2-412b-a37c-8e56ab881d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
